"""PyTorch Dataset for IRF prediction from Zarr data.

This module provides a PyTorch Dataset that loads IRF data from Zarr arrays
generated by the data pipeline. It supports:
- Loading theta (parameters) and IRFs from per-world Zarr directories
- Train/val/test splits via manifest.json
- Mixed-world batches via custom collate function
- Efficient chunked loading from Zarr
"""

import json
from pathlib import Path
from typing import Any, Literal

import numpy as np
import torch
import zarr
from torch.utils.data import Dataset


class IRFDataset(Dataset):
    """PyTorch Dataset for IRF prediction.

    Loads parameters (theta) and IRFs from Zarr arrays per world_id.
    Supports train/val/test splits defined in splits.json.

    Data format:
    - theta: (n_samples, n_params) - parameters in natural units
    - irfs: (n_samples, n_shocks, H+1, 3) - IRF for each shock

    Each sample returns a dict with:
    - theta: (n_params,) parameter vector
    - irf: (n_shocks, H+1, 3) IRF array for all shocks
    - world_id: str, simulator identifier
    - sample_idx: int, original sample index (for debugging)
    """

    def __init__(
        self,
        zarr_root: str | Path,
        world_ids: list[str],
        split: Literal["train", "val", "test_interpolation", "test_extrapolation_slice", "test_extrapolation_corner"] = "train",
    ):
        """Initialize IRF dataset.

        Args:
            zarr_root: Path to dataset root (e.g., "datasets/v1.0-dev")
            world_ids: List of world identifiers to load (e.g., ["nk", "var", "lss"])
            split: Which split to load ("train", "val", or "test_*")

        Raises:
            FileNotFoundError: If zarr_root or world directories don't exist
            ValueError: If split not found in splits.json
        """
        self.zarr_root = Path(zarr_root)
        self.world_ids = world_ids
        self.split = split

        if not self.zarr_root.exists():
            raise FileNotFoundError(f"Dataset root not found: {self.zarr_root}")

        # Load manifest to get metadata
        manifest_path = self.zarr_root / "manifest.json"
        if not manifest_path.exists():
            raise FileNotFoundError(f"Manifest not found: {manifest_path}")

        with open(manifest_path, "r") as f:
            self.manifest = json.load(f)

        # Load data for each world
        self.world_data: dict[str, dict[str, Any]] = {}
        self.samples: list[dict[str, Any]] = []

        for world_id in world_ids:
            world_dir = self.zarr_root / world_id
            if not world_dir.exists():
                raise FileNotFoundError(f"World directory not found: {world_dir}")

            # Load Zarr arrays
            theta_zarr = zarr.open(world_dir / "theta.zarr", mode="r")
            irfs_zarr = zarr.open(world_dir / "irfs.zarr", mode="r")

            # Load splits for this world
            splits_path = world_dir / "splits.json"
            if not splits_path.exists():
                raise FileNotFoundError(f"Splits file not found: {splits_path}")

            with open(splits_path, "r") as f:
                splits = json.load(f)

            if split not in splits:
                raise ValueError(f"Split '{split}' not found in {splits_path}")

            split_indices = np.array(splits[split])

            # Store world data
            self.world_data[world_id] = {
                "theta": theta_zarr,
                "irfs": irfs_zarr,
                "split_indices": split_indices,
                "n_params": theta_zarr.shape[1],
                "n_shocks": irfs_zarr.shape[1],
                "H": irfs_zarr.shape[2] - 1,  # IRF shape is (n_shocks, H+1, 3)
            }

            # Create sample list: (world_id, local_idx, global_idx)
            for local_idx, global_idx in enumerate(split_indices):
                self.samples.append({
                    "world_id": world_id,
                    "local_idx": local_idx,
                    "global_idx": int(global_idx),
                })

    def __len__(self) -> int:
        """Return total number of samples across all worlds."""
        return len(self.samples)

    def __getitem__(self, idx: int) -> dict[str, Any]:
        """Get a single sample.

        Args:
            idx: Index into the combined dataset

        Returns:
            Dictionary with keys:
            - theta: (n_params,) parameter tensor
            - irf: (n_shocks, H+1, 3) IRF tensor
            - world_id: str
            - sample_idx: int (original index in world's data)
        """
        sample_info = self.samples[idx]
        world_id = sample_info["world_id"]
        global_idx = sample_info["global_idx"]

        world_data = self.world_data[world_id]

        # Load from Zarr (automatically converts to numpy)
        theta = world_data["theta"][global_idx]  # (n_params,)
        irf = world_data["irfs"][global_idx]  # (n_shocks, H+1, 3)

        # Convert to torch tensors
        theta = torch.from_numpy(np.array(theta)).float()
        irf = torch.from_numpy(np.array(irf)).float()

        return {
            "theta": theta,
            "irf": irf,
            "world_id": world_id,
            "sample_idx": global_idx,
        }

    def get_world_info(self, world_id: str) -> dict[str, Any]:
        """Get metadata for a specific world.

        Args:
            world_id: World identifier

        Returns:
            Dictionary with n_params, n_shocks, H
        """
        if world_id not in self.world_data:
            raise ValueError(f"World '{world_id}' not in dataset")

        return {
            "n_params": self.world_data[world_id]["n_params"],
            "n_shocks": self.world_data[world_id]["n_shocks"],
            "H": self.world_data[world_id]["H"],
        }

    def get_param_manifest(self, world_id: str) -> dict[str, Any]:
        """Get parameter manifest for a world from the dataset manifest.

        Args:
            world_id: World identifier

        Returns:
            Parameter manifest dict with names, bounds, defaults, etc.
        """
        if world_id not in self.manifest["simulators"]:
            raise ValueError(f"World '{world_id}' not in manifest")

        return self.manifest["simulators"][world_id]["param_manifest"]


def collate_mixed_worlds(batch: list[dict[str, Any]]) -> dict[str, Any]:
    """Collate function for mixed-world batches.

    Handles variable-length theta vectors by padding to max length in batch.
    Groups samples by world_id for easier per-world processing.

    Args:
        batch: List of sample dicts from __getitem__

    Returns:
        Dictionary with:
        - theta: (batch_size, max_n_params) padded parameter tensor
        - irf: (batch_size, max_n_shocks, H+1, 3) padded IRF tensor
        - world_ids: list[str] of length batch_size
        - sample_indices: list[int] of length batch_size
        - theta_mask: (batch_size, max_n_params) boolean mask for valid params
        - irf_mask: (batch_size, max_n_shocks) boolean mask for valid shocks
        - world_groups: dict mapping world_id to list of batch indices
        - shock_idx: (batch_size,) tensor of shock indices for each sample
        - eps_sequence: (batch_size, T, max_n_shocks) shock sequence (if present)
        - history: (batch_size, k, n_obs) observable history (if present)
    """
    batch_size = len(batch)

    # Extract components
    thetas = [sample["theta"] for sample in batch]
    irfs = [sample["irf"] for sample in batch]
    world_ids = [sample["world_id"] for sample in batch]
    sample_indices = [sample["sample_idx"] for sample in batch]

    # Determine max dimensions
    max_n_params = max(theta.shape[0] for theta in thetas)
    max_n_shocks = max(irf.shape[0] for irf in irfs)
    H_plus_1 = irfs[0].shape[1]  # Should be same for all (H+1)

    # Pad theta
    theta_padded = torch.zeros(batch_size, max_n_params)
    theta_mask = torch.zeros(batch_size, max_n_params, dtype=torch.bool)

    for i, theta in enumerate(thetas):
        n_params = theta.shape[0]
        theta_padded[i, :n_params] = theta
        theta_mask[i, :n_params] = True

    # Pad IRFs
    irf_padded = torch.zeros(batch_size, max_n_shocks, H_plus_1, 3)
    irf_mask = torch.zeros(batch_size, max_n_shocks, dtype=torch.bool)

    for i, irf in enumerate(irfs):
        n_shocks = irf.shape[0]
        irf_padded[i, :n_shocks, :, :] = irf
        irf_mask[i, :n_shocks] = True

    # Group by world_id for easier per-world operations
    world_groups: dict[str, list[int]] = {}
    for i, world_id in enumerate(world_ids):
        if world_id not in world_groups:
            world_groups[world_id] = []
        world_groups[world_id].append(i)

    # Prepare shock_idx: For multi-shock IRFs, randomly select one shock per sample
    # This assumes IRF training where we predict response to a single shock
    shock_idx = torch.zeros(batch_size, dtype=torch.long)
    for i in range(batch_size):
        n_shocks = irfs[i].shape[0]
        # For now, use first shock (can be randomized during training)
        shock_idx[i] = 0

    result = {
        "theta": theta_padded,
        "irf": irf_padded,
        "world_ids": world_ids,
        "sample_indices": sample_indices,
        "theta_mask": theta_mask,
        "irf_mask": irf_mask,
        "world_groups": world_groups,
        "shock_idx": shock_idx,
    }

    # Add optional fields if present in batch
    if "eps_sequence" in batch[0]:
        eps_sequences = [sample["eps_sequence"] for sample in batch]
        # Determine max time dimension
        T = max(eps.shape[0] for eps in eps_sequences)
        eps_padded = torch.zeros(batch_size, T, max_n_shocks)
        for i, eps in enumerate(eps_sequences):
            T_i, n_shocks_i = eps.shape
            eps_padded[i, :T_i, :n_shocks_i] = eps
        result["eps_sequence"] = eps_padded

    if "history" in batch[0]:
        histories = [sample["history"] for sample in batch]
        # Determine max history length
        k = max(h.shape[0] for h in histories)
        n_obs = histories[0].shape[1]
        history_padded = torch.zeros(batch_size, k, n_obs)
        history_mask = torch.zeros(batch_size, k, dtype=torch.bool)
        for i, h in enumerate(histories):
            k_i = h.shape[0]
            history_padded[i, :k_i, :] = h
            history_mask[i, :k_i] = True
        result["history"] = history_padded
        result["history_mask"] = history_mask

    return result


def collate_single_world(batch: list[dict[str, Any]]) -> dict[str, Any]:
    """Collate function for single-world batches (no padding needed).

    Use this when you know all samples in a batch are from the same world.

    Args:
        batch: List of sample dicts from __getitem__

    Returns:
        Dictionary with:
        - theta: (batch_size, n_params) stacked parameters
        - irf: (batch_size, n_shocks, H+1, 3) stacked IRFs
        - world_id: str (same for all samples)
        - sample_indices: list[int]
        - shock_idx: (batch_size,) tensor of shock indices
        - eps_sequence: (batch_size, T, n_shocks) shock sequence (if present)
        - history: (batch_size, k, n_obs) observable history (if present)
    """
    batch_size = len(batch)

    # Verify all from same world
    world_ids = [sample["world_id"] for sample in batch]
    if len(set(world_ids)) > 1:
        raise ValueError(
            f"collate_single_world called on mixed-world batch: {set(world_ids)}"
        )

    world_id = world_ids[0]

    # Stack directly (no padding needed)
    theta = torch.stack([sample["theta"] for sample in batch])  # (batch_size, n_params)
    irf = torch.stack([sample["irf"] for sample in batch])  # (batch_size, n_shocks, H+1, 3)
    sample_indices = [sample["sample_idx"] for sample in batch]

    # Prepare shock_idx: For multi-shock IRFs, select one shock per sample
    n_shocks = irf.shape[1]
    shock_idx = torch.zeros(batch_size, dtype=torch.long)
    # For now, use first shock (can be randomized during training)

    result = {
        "theta": theta,
        "irf": irf,
        "world_id": world_id,
        "sample_indices": sample_indices,
        "shock_idx": shock_idx,
    }

    # Add optional fields if present
    if "eps_sequence" in batch[0]:
        result["eps_sequence"] = torch.stack([sample["eps_sequence"] for sample in batch])

    if "history" in batch[0]:
        result["history"] = torch.stack([sample["history"] for sample in batch])
        if "history_mask" in batch[0]:
            result["history_mask"] = torch.stack([sample["history_mask"] for sample in batch])

    return result
